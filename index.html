<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700
    }
    
    venue {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-style: italic
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 40px;
    }

    email {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }
    
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
  <title>Thomas Wolf</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Thomas Wolf</name>
                <br>
                <email>thomwolf [at] gmail [dot] com</email>
              </p>
              <p>I lead the Science Team at <a href="https://huggingface.co">Huggingface Inc.</a>, a Brooklyn-based startup working on Natural Language Generation and Natural Language Understanding.</p>
              <p>I’ve been programming since I was 10, writing video games and interactive software in <a href="https://www.ticalc.org/archives/files/fileinfo/155/15599.html">Assembly</a> and <a href="https://github.com/thomwolf/Magic-Sand">C/C++</a> but my first career was actually in Physics rather than Computer Science.</p>
              <p>After graduating from <a href="https://www.polytechnique.edu/en">Ecole Polytechnique</a> (Paris, France), I worked on laser-plasma interactions at the <a href="http://bella.lbl.gov/">BELLA Center of the Lawrence Berkeley National Laboratory</a> (Berkeley, CA). Got accepted for a PhD at <a href="https://web.mit.edu/">MIT</a> (Cambridge, MA) but ended up doing my PhD in Statistical/Quantum physics at <a href="http://www.upmc.fr/en/">Sorbonne University</a> and <a href="https://www.espci.fr/en/">ESPCI</a> (Paris, France), working on superconducting materials for the French DARPA (<a href="https://www.defense.gouv.fr/english/dga">DGA</a>) and <a href="https://www.thalesgroup.com/en">Thales</a>. After my PhD, I needed a change from the long time scale of experiments in physics and ended up totally changing direction. I joined an IP Law firm, <a href="https://www.plass.com/en">Cabinet Plasseraud</a> (Paris, France), got a law degree from <a href="http://www.pantheonsorbonne.fr/">Pantheon Sorbonne University</a> and worked as a <a href="https://patentepi.com/en/">European Patent Attorney</a> for 5 years, assisting a portfolio of startups and big companies to build and defend their Intellectual Property assets.</p>
              <p>In 2015, I was consulting for many Deep-Learning/AI/ML startups and they made me discover the maths behind the new ML/AI revolution. I realised that most of these methods, equations and tools were just re-branded statistical physics approaches which fueled my interest for Machine Learning and Deep Learning. I started my online education in AI/ML reading <a href="data/Thom_wolf_reading_list.txt">books and following online courses</a>. About year later, one of my friend asked me if I wanted to join <a href="https://huggingface.co/">his startup</a> to build a science team, and there I was, doing science again and having a lot of fun!
              </p>
              <p align=center>
                <a href="mailto:thomwolf@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://medium.com/@Thomwolf">Medium</a> &nbsp/&nbsp
                <a href="https://twitter.com/Thom_wolf">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/thomwolf">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/thomas-wolf-a056857/">LinkedIn</a>
              </p>
            </td>
            <td width="33%">
              <img src="images/Thom_photo_circle.jpg">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I'm interested in Natural Language Processing, Deep Learning and Computational Linguistics. Much of my research is about Natural Language Generation (mostly) and Natural Language Understanding (as a tool for better generation).
              </p>
              <p>
                You can find some details in <a href="">this interview</a> I gave to Gengo.AI's Daniel Smith and where I discussed the work we do at Huggingface, current trends in AI/NLP and my unusual background.
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Invited Talks and News</heading>
            </td>
          </tr>
        </table>
        <ul>
            <li>On <strong>September 7, 2019</strong>, I will give a talk in the AI Assistant Summit track at <a href="https://www.re-work.co/dl-london-2019">Re-Work Deep Learning</a> in London, UK.
            <li>On <strong>September 7, 2019</strong>, I gave a talk on 
                Transfer learning in NLP at the <a href="https://fwdays.com/en/event/data-science-fwdays-2019">Data Science fwdays'19</a> in Kyiv, Ukraine [<a href="https://docs.google.com/presentation/d/1VOy8aefH0bypwk35soEuQ0geW0RVrr-o6I6O8IKXmK4">slides</a>].
            <li>On <strong>June 6, 2019</strong>, I co-organized a workshop on Methods for Optimizing and Evaluating Neural Language Generation (<a href="http://neuralgen.io">NeuralGen</a>), together with Antoine Bosselut, Marjan Ghazvininejad, Srinivasan Iyer, Urvashi Khandelwal, Hannah Rashkin and Asli Celikyilmaz, co-located with <a href="https://naacl2019.org/program/workshops/#neuralgen">NAACL 2019</a> [<a href="http://neuralgen.io">website</a>].
            <li>On <strong>June 2nd, 2019</strong>, I gave a tutorial on Transfer Learning in Natural Language Processing, together with Sebastian Ruder, Swabha Swayamdipta and Matthew Peters at <a href="https://naacl2019.org/program/tutorials/#t4-transfer-learning-in-natural-language-processing">NAACL 2019</a> [<a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc">slides</a>].
            </li>
            <li>On <strong>March 1st, 2019</strong>, I gave a talk at the <a href="http://ilps.science.uva.nl/">ILPS lab</a> of the University of Amsterdam on Hierarchical Multi-tasking for learning embeddings from semantic tasks as part of the ILPS Monthly talks [<a href="data/ILPS_talk_2019_03_01.pdf">slides</a>].
          </li>
          <li>On <strong>January 30, 2019</strong>, I gave a talk at the <a href="https://www.meetup.com/Deep-Learning-Paris-Meetup/">Deep Learning Meetup</a> in Paris [<a href="data/Meetup_Deep_Learning_Paris_2019_01_30.pdf">slides</a>]
          </li>
          <li>On <strong>January 22, 2019</strong>, I gave a talk at the NYU Center for Data Science on Transfer Learning Approaches to Natural Language Generation [<a href="data/Amsterdam_Uni_2019_01_18 - final.pdf">see my UvA slides</a>]
          </li>
          <li>On <strong>January 18, 2019</strong>, I gave a talk at the University of Amsterdam as part of the <a href="https://www.meetup.com/SEA-Search-Engines-Amsterdam/events/254751150/?rv=ea2_v2">SEA Meetups</a> on a Transfer Learning Approach to Open-Domain Neural Network Dialog Agents [<a href="data/Amsterdam_Uni_2019_01_18 - final.pdf">slides</a>]
          </li>
          <li>On <strong>January 11, 2019</strong>, I gave a talk at Utrecht University as part of the <a href="https://www.uu.nl/en/events/dscc-central-topic-seminar-5-machine-learning-applications-in-chatbot-and-language-processing">Data Science & Complexity Centre (DSCC) Central Topic Seminars</a> on recent developments in Neural Network Based Dialogue Agents focusing on the use of Transfer Learning for dialog generation [<a href="data/Utrecht_Uni_2019_01_11 - final - small.pdf">slides</a>]
          </li>
          <li>In <strong>December 2018</strong>, I gave a talk during the <a href="https://wecnlpsummit2018rsvp.splashthat.com/">NeurIPS 2018 Competition Track</a> at part of the Winners talks & spotlights, discussing our solution to the Conversational Intelligence Challenge 2 (ConvAI2) [<a href="data/NeurIPS2018_competition_HuggingFace.pdf">slides</a>]  [<a href="data/TransferTransfo_final.pdf">paper</a>]
          </li>
          <li>In <strong>September 2018</strong>, I gave a talk at <a href="https://wecnlpsummit2018rsvp.splashthat.com/">The first annual WeCNLP Summit 2018</a> on a novel architecture and training scheme for chit-chat dialog systems [<a href="data/WeCNLP_2018.pdf">slides</a>]
          </li>
          <li>In <strong>September 2018</strong>, I gave <a href="https://www.meetup.com/Paris-NLP/events/xzstdqyxmbjc/">a talk at Paris NLP</a> on Neural networks based dialog agents: going beyond the seq2seq model [<a href="data/ParisNLP_2018.pdf">slides</a>]
          </li>
          <li>In <strong>October 2017</strong>, I gave a talk at <a href="https://franceisai.com/conferences/conference-2017#">France is AI 2017</a> on NeuralCoref, a neural coreference system for conversational agents [<a href="data/France-is-AI.pdf">slides</a>]
          </li>
        </ul>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Blog</heading>
              <p>
                I like to explain clearly what I have learned and this has lead to a few blog posts that were quite interesting to other as well I guess (they totalise over a quarter million views at the end of 2018). I will try to continue writing things like that when I find the time. I used to be a teacher during my PhD and I do miss teaching. Blogging is my substitute.
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/training.jpeg" alt="Training Neural Nets on Larger Batches" width="160"></td>
            <td width="75%" valign="center">
              <p>
                <a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">
                  <papertitle>💥 Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups</papertitle>
                </a>
                <p>I've spent most of 2018 training models that could barely fit 1-4 samples/GPU.
                  But SGD usually needs more than few samples/batch for decent results.
                  I wrote a post gathering practical tips I use, from simple tricks to multi-GPU code & distributed setups</p>
              </p>
            </td>
          </tr>
        </table>

        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/meaning.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e">
                <papertitle>⛵ Learning Meaning in Natural Language Processing — The Semantics Mega-Thread</papertitle>
              </a>
              <p>A summary, overview and map of a huge discussion on learning meaning in NLP that happened on Twitter in August 2018 with more than a 100 comments and great inputs from Matt Gardner, Yoav Goldberg, Sam Bowman, Emily M. Bender, Graham Neubig, Jeremy Howard, Tal Linzen, Jacob Andreas, Ryan D. Cotterell ...</p>
            </p>
        </td>
          </tr>

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/100_times.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/100-times-faster-natural-language-processing-in-python-ee32033bdced">
                <papertitle>🚀 100 Times Faster Natural Language Processing in Python</papertitle>
              </a>
              <p>How you can make your Python NLP module 50-100 times faster by use spaCy's internals and a bit of Cython magic! Womes with a Jupyter notebook with examples processing over 80 millions words per sec.
                </p>
            </p>
        </td>
          </tr>

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/words.png'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a">
                <papertitle>📚The Current Best of Universal Word Embeddings and Sentence Embeddings</papertitle>
              </a>
              <p>A post summarizing recent developments in Universal Word/Sentence Embeddings that happend over 2017/early-2018 and future trends. With ELMo, InferSent, Google's Universal Sentence embeddings, learning by multi-tasking... Written with <a href="https://twitter.com/SanhEstPasMoi">Victor Sanh</a>.
                </p>
            </p>
        </td>
          </tr>

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/meta_learning.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a">
                <papertitle>🐣 From zero to research — An introduction to Meta-learning</papertitle>
              </a>
              <p>To introduce <a href="https://arxiv.org/abs/1803.10631">the work we presented at ICLR 2018</a>, I drafted a visual & intuitive introduction to Meta-Learning. In this post, I start by explaining what’s meta-learning in a very visual and intuitive way. Then, we code a meta-learning model in PyTorch and I share some of the lessons learned on this project.
                </p>
            </p>
        </td>
          </tr>


          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/train_coref.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/how-to-train-a-neural-coreference-model-neuralcoref-2-7bb30c1abdfe">
                <papertitle>✨How to train a neural coreference model— Neuralcoref 2</papertitle>
              </a>
              <p>A post describing the internals of <a href="https://github.com/huggingface/neuralcoref">NeuralCoref</a>. Neuralcoref is designed to strike a good balance between accuracy and speed/simplicity, using a rule-based mention detection module, a constrained number of features and a simple feed-forward neural network. This post describes how the coreference resolution system works and how to train it.
            </p>
        </td>
          </tr>

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/emotions.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/how-to-train-a-neural-coreference-model-neuralcoref-2-7bb30c1abdfe">
                <papertitle>Understanding emotions — from Keras to pyTorch
                </papertitle>
              </a>
              <p>A post accompanying our open-sourcing of <a href="https://github.com/huggingface/torchMoji">torchMoji</a>, a PyTorch adaptation of MIT's DeepMoji model. In this post, I detail several points that arose during the reimplementation of a Keras model in PyTorch: how to make a custom pyTorch LSTM with custom activation functions,
                how the PackedSequence object works and is built,
                how to convert an attention layer from Keras to pyTorch,
                how to load your data in pyTorch: DataSets and smart Batching,
                how to reproduce Keras weights initialization in pyTorch.
                </p>
            </p>
        </td>
          </tr>

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/sota_coref.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30">
                <papertitle>State-of-the-art neural coreference resolution for chatbots
                </papertitle>
              </a>
              <p>A post accompanying our open-sourcing of <a href="https://github.com/huggingface/neuralcoref">NeuralCoref</a>. It comprise an introduction to the field of co-reference resolution and describes how a coreference resolution system works in practice.
                </p>
            </p>
        </td>
          </tr>
        </table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Open-sourced Projects</heading>
              <p>
                I also like to open-source my code base when I think it can be interesting to others. This has led to very interesting collaborations in the past. These few projects totalize a few thousand github stars and I am always happy when I see people coming with great PR on them to share their developments and ideas.
              </p>
              </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/sandbox.jpg" alt="sandbox"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://github.com/thomwolf/Magic-Sand">
                  <papertitle>Magic Sandbox</papertitle>
                </a>
                <p>
                  Magic Sand is a software for operating an augmented reality sandbox like the <a href="https://arsandbox.ucdavis.edu/">Augmented Reality Sandbox</a> developped by UC Davis. This project comprises the C++ codebase build on <a href="">OpenFrameWorks</a> and a <a href="https://imgur.com/gallery/Q86wR">tutorial</a> to build the hardware (see also the associated <a href="https://www.reddit.com/r/DIY/comments/4v1gfi/a_magic_sandbox_i_made_for_my_3_yo_sons_birthday/">reddit thread</a>)
                </p>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/neuralcoref.png"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://github.com/huggingface/neuralcoref">
                  <papertitle>✨NeuralCoref: Coreference Resolution in spaCy with Neural Networks.</papertitle>
                </a>
                <p>
NeuralCoref is a pipeline extension for spaCy 2.0 that annotates and resolves coreference clusters using a neural network. NeuralCoref is production-ready, integrated in spaCy's NLP pipeline and easily extensible to new training datasets.
                </p>
                <p>
NeuralCoref is written in Cython & Python and comes with pre-trained statistical models for English. It can be trained in other languages.
                </p>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/torchmoji.jpeg"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://github.com/huggingface/torchMoji">
                  <papertitle>😇 TorchMoji</papertitle>
                </a>
                <p>
                  TorchMoji is a pyTorch implementation of the <a href="https://github.com/bfelbo/DeepMoji">DeepMoji</a> model developped by Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad Rahwan and Sune Lehmann.

                  This model trained on 1.2 billion tweets with emojis to understand how language is used to express emotions. Through transfer learning the model can obtain state-of-the-art performance on many emotion-related text modeling tasks. See the <a href="https://arxiv.org/abs/1708.00524">paper</a> for more details.
                  
                                  </p>
              </p>
            </td>
          </tr>

        <tr>
          <td width="25%"><img src="images/openai_gpt.png"></td>
          <td width="75%" valign="top">
            <p>
              <a href="https://github.com/huggingface/pytorch-openai-transformer-lm">
                <papertitle>PyTorch implementation of OpenAI's Finetuned Transformer Language Model.</papertitle>
              </a>
              <p>
                A PyTorch implementation of the <a href="https://github.com/openai/finetune-transformer-lm">TensorFlow code</a> provided with OpenAI's paper <a href="https://blog.openai.com/language-unsupervised">Improving Language Understanding by Generative Pre-Training</a> by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.

                This implementation comprises a script to load in the PyTorch model the weights pre-trained by the authors with the TensorFlow implementation.
                
              </p>
            </p>
          </td>
        </tr>
  
        
        <tr>
          <td width="25%"><img src="images/alien-monster.png"></td>
          <td width="75%" valign="top">
            <p>
              <a href="https://github.com/huggingface/pytorch-transformers">
                <papertitle>PyTorch Transformers.</papertitle>
              </a>
              <p>
                PyTorch-Transformers (formerly known as pytorch-pretrained-bert) is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).
              </p>
            </p>
          </td>
        </tr>

      </table>

      


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
            <heading>Publications</heading>
          </td>
        </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tr>
            <td width="25%"><img src="images/publi_TransferTransfo.png"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://www.aclweb.org/anthology/N19-5004">
                  <papertitle>Transfer Learning in Natural Language Processing</papertitle>
                </a>
                <br>
                Sebastian Ruder, Matthew E Peters, Swabha Swayamdipta, <strong>Thomas Wolf</strong> (all authors contributed equally)
                <br>
                <venue>NAACL 2019 (Tutorial)</venue>
                  <p>
                  <br> The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.
                </p>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/publi_TransferTransfo.png"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://www.aclweb.org/anthology/P19-1608">
                  <papertitle>Large-Scale Transfer Learning for Natural Language Generation</papertitle>
                </a>
                <br>
                Sergey Golovanov, Rauf Kurbanov, Sergey Nikolenko, Kyryl Truskovskyi, Alexander Tselousov, <strong>Thomas Wolf</strong> (all authors contributed equally)
                <br>
                <venue>ACL 2019</venue>
                  <p>
                  <br> Large-scale pretrained language models define state of the art in natural language processing, achieving outstanding performance on a variety of tasks. We study how these architectures can be applied and adapted for natural language generation, comparing a number of architectural and training schemes. We focus in particular on open-domain dialog as a typical high entropy generation task, presenting and comparing different architectures for adapting pretrained models with state of the art results.
                </p>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/publi_TransferTransfo.png"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://huggingface.co/bert-syntax/extending-bert-syntax.pdf">
                  <papertitle>Some additional experiments extending the tech report “Assessing BERT’s Syntactic Abilities” by Yoav Goldberg</papertitle>
                </a>
                <br>
                <strong>Thomas Wolf</strong>
                <br>
                <venue>Tech report</venue>
                  <p>
                  <br> This document report a few additional experiments extending Yoav Goldberg’s tech report ”Assessing BERT’s Syntactic Abilities” by evaluating the OpenAI Generative Pre-trained Transformer of Radford et al. [2018]1 which is a Transformer model with an architecture highly similar to BERT (see discussion below) but has been pre-trained with a Language Modeling objective on the Toronto Book Corpus [Zhu et al., 2015] only, and evaluating BERT when it is only supplied with a prefix.
                </p>
              </p>
            </td>
          </tr>

      <tr>
            <td width="25%"><img src="images/publi_TransferTransfo.png"></td>
            <td width="75%" valign="top">
              <p>
                <a href="http://alborz-geramifard.com/workshops/nips18-Conversational-AI/Main.html">
                  <papertitle>TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents</papertitle>
                </a>
                <br>
                <strong>Thomas Wolf</strong>, Victor Sanh, Julien Chaumond and Clement Delangue
                <br>
                <venue>NeurIPS CAI workshop 2018</venue>
                  <p>
                  <br> We introduce a new approach to data-driven dialogue systems (e.g. chatbots) called TransferTransfo which is a combination of aTransferlearning based train-ing scheme and a high-capacity generativeTransfo-rmer model.  Fine-tuning isperformed by using a multi-task objective which combines several unsupervised pre-diction tasks. The resulting fine-tuned model shows strong improvements over thecurrent state-of-the-art end-to-end conversational models like memory augmentedseq2seq and information-retrieval models. On the privately held PERSONA-CHATdataset of the Conversational Intelligence Challenge 2, this approach obtains anew state-of-the-art, respectively pushing the perplexity, Hits@1 and F1 metrics to 16.28 (45% absolute improvement),80.7 (46% absolute improvement) and 19.5 (20% absolute improvement).
                </p>
              </p>
            </td>
          </tr>

      <tr>
            <td width="25%"><img src="images/publi_hmtl.png"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://arxiv.org/abs/1811.06031">
                  <papertitle>A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks
                    </papertitle>
                </a>
                <br>
                Victor Sanh, <strong>Thomas Wolf</strong>, Sebastian Ruder
                <br>
                <venue>AAAI 2019</venue>
                </p>
                <p>
                  <br> Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.

                </p>
              </p>
            </td>
          </tr>

      <tr>
          <td width="25%"><img src="images/publi_metalearning.png"></td>
          <td width="75%" valign="top">
            <p>
              <a href="https://arxiv.org/abs/1803.10631">
                <papertitle>Meta-Learning a Dynamical Language Model</papertitle>
              </a>
              <br>
              <strong>Thomas Wolf</strong>, Julien Chaumond, Clement Delangue, 2018
              <br>
              <venue>Workshop track - ICLR 2018</venue>
              </p>
              <p>
                <br> We consider the task of word-level language modeling and study the possibility of combining hidden-states-based short-term representations with medium-term representations encoded in dynamical weights of a language model. Our work extends recent experiments on language models with dynamically evolving weights by casting the language modeling problem into an online learning-to-learn framework in which a meta-learner is trained by gradient-descent to continuously update a language model weights.
              </p>
            </p>
          </td>
        </tr>

        <tr>
            <td width="25%"><img src="images/publi_metalearning_acl.png"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://arxiv.org/abs/1805.05758">
                  <papertitle>Continuous Learning in a Hierarchical Multiscale Neural Network
                    </papertitle>
                </a>
                <br>
                <strong>Thomas Wolf</strong>, Julien Chaumond, Clement Delangue, 2018
                <br>
                <venue>ACL 2018</venue>
                </p>
                <p>
                  <br> We reformulate the problem of encoding a multi-scale representation of a sequence in a language model by casting it in a continuous learning framework. We propose a hierarchical multi-scale language model in which short time-scale dependencies are encoded in the hidden state of a lower-level recurrent neural network while longer time-scale dependencies are encoded in the dynamic of the lower-level network by having a meta-learner update the weights of the lower-level neural network in an online meta-learning fashion. We use elastic weights consolidation as a higher-level to prevent catastrophic forgetting in our continuous learning framework.

                </p>
              </p>
            </td>
          </tr>
  

      </table>




        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  <a href="https://github.com/jonbarron/jonbarron_website"><strong>Created from Jonathan T. Barron's template</strong></a>
                  </font>
              </p>
            </td>
          </tr>
        </table>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
