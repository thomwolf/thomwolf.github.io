<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700
    }
    
    venue {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-style: italic
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 40px;
    }

    email {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }
    
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
  <title>Thomas Wolf</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Thomas Wolf</name>
                <br>
                <email>thomwolf [at] gmail [dot] com</email>
              </p>
              <p>I lead the Science Team at <a href="https://huggingface.co">Huggingface Inc.</a>, a Brooklyn-based startup working on Natural Language Generation and Natural Language Understanding.</p>
              <p>I‚Äôve been programming since I was 10, writing video games and interactive software in <a href="https://www.ticalc.org/archives/files/fileinfo/155/15599.html">Assembly</a> and <a href="https://github.com/thomwolf/Magic-Sand">C/C++</a> but my first career was actually in Physics rather than Computer Science.</p>
              <p>After graduating from <a href="https://www.polytechnique.edu/en">Ecole Polytechnique</a> (Paris, France), I worked on laser-plasma interactions at the <a href="http://bella.lbl.gov/">BELLA Center of the Lawrence Berkeley National Laboratory</a> (Berkeley, CA). Got accepted for a PhD at <a href="https://web.mit.edu/">MIT</a> (Cambridge, MA) but ended up doing my PhD in Statistical/Quantum physics at <a href="http://www.upmc.fr/en/">Sorbonne University</a> and <a href="https://www.espci.fr/en/">ESPCI</a> (Paris, France), working on superconducting materials for the French DARPA (<a href="https://www.defense.gouv.fr/english/dga">DGA</a>) and <a href="https://www.thalesgroup.com/en">Thales</a>. After my PhD, I needed a change from the long time scale of experiments in physics and ended up totally changing direction. I joined an IP Law firm, <a href="https://www.plass.com/en">Cabinet Plasseraud</a> (Paris, France), got a law degree from <a href="http://www.pantheonsorbonne.fr/">Pantheon Sorbonne University</a> and worked as a <a href="https://patentepi.com/en/">European Patent Attorney</a> for 5 years, assisting a portfolio of startups and big companies to build and defend their Intellectual Property assets.</p>
              <p>In 2015, I was consulting for many Deep-Learning/AI/ML startups and they made me discover the maths behind the new ML/AI revolution. I realised that most of these methods, equations and tools were just re-branded statistical physics approaches which fueled my interest for Machine Learning and Deep Learning. I started my online education in AI/ML reading <a href="data/Thom_wolf_reading_list.txt">books and following online courses</a>. About year later, one of my friend asked me if I wanted to join <a href="https://huggingface.co/">his startup</a> to build a science team, and there I was, doing science again and having a lot of fun!
              </p>
              <p align=center>
                <a href="mailto:thomwolf@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://medium.com/@Thomwolf">Medium</a> &nbsp/&nbsp
                <a href="https://twitter.com/Thom_wolf">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/thomwolf">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/thomas-wolf-a056857/">LinkedIn</a>
              </p>
            </td>
            <td width="33%">
              <img src="images/Thom_photo_circle.jpg">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I'm interested in Natural Language Processing, Deep Learning and Computational Linguistics. Much of my research is about Natural Language Generation (mostly) and Natural Language Understanding (as a tool for better generation).
              </p>
              <p>
                You can find some details in <a href="">this interview</a> I gave to Gengo.AI's Daniel Smith and where I discussed the work we do at Huggingface, current trends in AI/NLP and my unusual background.
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Invited Talks</heading>
            </td>
          </tr>
        </table>
        <ul>
          <li>On <strong>March 1st, 2019</strong>, I'll give a talk at the <a href="http://ilps.science.uva.nl/">ILPS lab</a> of the University of Amsterdam as part of the ILPS Monthly talks.
          </li>
          <li>On <strong>January 30, 2019</strong>, I gave a talk at the <a href="https://www.meetup.com/Deep-Learning-Paris-Meetup/">Deep Learning Meetup</a> in Paris [<a href="data/Meetup_Deep_Learning_Paris_2019_01_30.pdf">slides</a>]
          </li>
          <li>On <strong>January 22, 2019</strong>, I gave a talk at the NYU Center for Data Science on Transfer Learning Approaches to Natural Language Generation [<a href="data/Amsterdam_Uni_2019_01_18 - final.pdf">see my UvA slides</a>]
          </li>
          <li>On <strong>January 18, 2019</strong>, I gave a talk at the University of Amsterdam as part of the <a href="https://www.meetup.com/SEA-Search-Engines-Amsterdam/events/254751150/?rv=ea2_v2">SEA Meetups</a> on a Transfer Learning Approach to Open-Domain Neural Network Dialog Agents [<a href="data/Amsterdam_Uni_2019_01_18 - final.pdf">slides</a>]
          </li>
          <li>On <strong>January 11, 2019</strong>, I gave a talk at Utrecht University as part of the <a href="https://www.uu.nl/en/events/dscc-central-topic-seminar-5-machine-learning-applications-in-chatbot-and-language-processing">Data Science & Complexity Centre (DSCC) Central Topic Seminars</a> on recent developments in Neural Network Based Dialogue Agents focusing on the use of Transfer Learning for dialog generation [<a href="data/Utrecht_Uni_2019_01_11 - final - small.pdf">slides</a>]
          </li>
          <li>In <strong>December 2018</strong>, I gave a talk during the <a href="https://wecnlpsummit2018rsvp.splashthat.com/">NeurIPS 2018 Competition Track</a> at part of the Winners talks & spotlights, discussing our solution to the Conversational Intelligence Challenge 2 (ConvAI2) [<a href="data/NeurIPS2018_competition_HuggingFace.pdf">slides</a>]  [<a href="data/TransferTransfo_final.pdf">paper</a>]
          </li>
          <li>In <strong>September 2018</strong>, I gave a talk at <a href="https://wecnlpsummit2018rsvp.splashthat.com/">The first annual WeCNLP Summit 2018</a> on a novel architecture and training scheme for chit-chat dialog systems [<a href="data/WeCNLP_2018.pdf">slides</a>]
          </li>
          <li>In <strong>September 2018</strong>, I gave <a href="https://www.meetup.com/Paris-NLP/events/xzstdqyxmbjc/">a talk at Paris NLP</a> on Neural networks based dialog agents: going beyond the seq2seq model [<a href="data/ParisNLP_2018.pdf">slides</a>]
          </li>
          <li>In <strong>October 2017</strong>, I gave a talk at <a href="https://franceisai.com/conferences/conference-2017#">France is AI 2017</a> on NeuralCoref, a neural coreference system for conversational agents [<a href="data/France-is-AI.pdf">slides</a>]
          </li>
        </ul>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Blog</heading>
              <p>
                I like to explain clearly what I have learned and this has lead to a few blog posts that were quite interesting to other as well I guess (they totalise about a quarter million views at the end of 2018). I will try to continue writing things like that when I find the time. I used to be a teacher during my PhD and I do miss teaching. Blogging is my substitute.
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/training.jpeg" alt="Training Neural Nets on Larger Batches" width="160"></td>
            <td width="75%" valign="center">
              <p>
                <a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">
                  <papertitle>üí• Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups</papertitle>
                </a>
                <p>I've spent most of 2018 training models that could barely fit 1-4 samples/GPU.
                  But SGD usually needs more than few samples/batch for decent results.
                  I wrote a post gathering practical tips I use, from simple tricks to multi-GPU code & distributed setups</p>
              </p>
            </td>
          </tr>
        </table>

        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/meaning.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e">
                <papertitle>‚õµ Learning Meaning in Natural Language Processing‚Ää‚Äî‚ÄäThe Semantics Mega-Thread</papertitle>
              </a>
              <p>A summary, overview and map of a huge discussion on learning meaning in NLP that happened on Twitter in August 2018 with more than a 100 comments and great inputs from Matt Gardner, Yoav Goldberg, Sam Bowman, Emily M. Bender, Graham Neubig, Jeremy Howard, Tal Linzen, Jacob Andreas, Ryan D. Cotterell ...</p>
            </p>
        </td>
          </tr>

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/100_times.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/100-times-faster-natural-language-processing-in-python-ee32033bdced">
                <papertitle>üöÄ 100 Times Faster Natural Language Processing in Python</papertitle>
              </a>
              <p>How you can make your Python NLP module 50-100 times faster by use spaCy's internals and a bit of Cython magic! Womes with a Jupyter notebook with examples processing over 80 millions words per sec.
                </p>
            </p>
        </td>
          </tr>

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/words.png'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a">
                <papertitle>üìöThe Current Best of Universal Word Embeddings and Sentence Embeddings</papertitle>
              </a>
              <p>A post summarizing recent developments in Universal Word/Sentence Embeddings that happend over 2017/early-2018 and future trends. With ELMo, InferSent, Google's Universal Sentence embeddings, learning by multi-tasking... Written with <a href="https://twitter.com/SanhEstPasMoi">Victor Sanh</a>.
                </p>
            </p>
        </td>
          </tr>

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/meta_learning.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a">
                <papertitle>üê£ From zero to research‚Ää‚Äî‚ÄäAn introduction to Meta-learning</papertitle>
              </a>
              <p>To introduce <a href="https://arxiv.org/abs/1803.10631">the work we presented at ICLR 2018</a>, I drafted a visual & intuitive introduction to Meta-Learning. In this post, I start by explaining what‚Äôs meta-learning in a very visual and intuitive way. Then, we code a meta-learning model in PyTorch and I share some of the lessons learned on this project.
                </p>
            </p>
        </td>
          </tr>


          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/train_coref.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/how-to-train-a-neural-coreference-model-neuralcoref-2-7bb30c1abdfe">
                <papertitle>‚ú®How to train a neural coreference model‚Äî Neuralcoref 2</papertitle>
              </a>
              <p>A post describing the internals of <a href="https://github.com/huggingface/neuralcoref">NeuralCoref</a>. Neuralcoref is designed to strike a good balance between accuracy and speed/simplicity, using a rule-based mention detection module, a constrained number of features and a simple feed-forward neural network. This post describes how the coreference resolution system works and how to train it.
            </p>
        </td>
          </tr>

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/emotions.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/how-to-train-a-neural-coreference-model-neuralcoref-2-7bb30c1abdfe">
                <papertitle>Understanding emotions‚Ää‚Äî‚Ääfrom Keras to pyTorch
                </papertitle>
              </a>
              <p>A post accompanying our open-sourcing of <a href="https://github.com/huggingface/torchMoji">torchMoji</a>, a PyTorch adaptation of MIT's DeepMoji model. In this post, I detail several points that arose during the reimplementation of a Keras model in PyTorch: how to make a custom pyTorch LSTM with custom activation functions,
                how the PackedSequence object works and is built,
                how to convert an attention layer from Keras to pyTorch,
                how to load your data in pyTorch: DataSets and smart Batching,
                how to reproduce Keras weights initialization in pyTorch.
                </p>
            </p>
        </td>
          </tr>

          <tr >
            <td width="25%">
              <div class="one">
                <img src='images/sota_coref.jpeg'>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30">
                <papertitle>State-of-the-art neural coreference resolution for chatbots
                </papertitle>
              </a>
              <p>A post accompanying our open-sourcing of <a href="https://github.com/huggingface/neuralcoref">NeuralCoref</a>. It comprise an introduction to the field of co-reference resolution and describes how a coreference resolution system works in practice.
                </p>
            </p>
        </td>
          </tr>
        </table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Open-sourced Projects</heading>
              <p>
                I also like to open-source my code base when I think it can be interesting to others. This has led to very interesting collaborations in the past. These few projects totalize a few thousand github stars and I am always happy when I see people coming with great PR on them to share their developments and ideas.
              </p>
              </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/sandbox.jpg" alt="sandbox"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://github.com/thomwolf/Magic-Sand">
                  <papertitle>Magic Sandbox</papertitle>
                </a>
                <p>
                  Magic Sand is a software for operating an augmented reality sandbox like the <a href="https://arsandbox.ucdavis.edu/">Augmented Reality Sandbox</a> developped by UC Davis. This project comprises the C++ codebase build on <a href="">OpenFrameWorks</a> and a <a href="https://imgur.com/gallery/Q86wR">tutorial</a> to build the hardware (see also the associated <a href="https://www.reddit.com/r/DIY/comments/4v1gfi/a_magic_sandbox_i_made_for_my_3_yo_sons_birthday/">reddit thread</a>)
                </p>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/neuralcoref.png"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://github.com/huggingface/neuralcoref">
                  <papertitle>‚ú®NeuralCoref: Coreference Resolution in spaCy with Neural Networks.</papertitle>
                </a>
                <p>
NeuralCoref is a pipeline extension for spaCy 2.0 that annotates and resolves coreference clusters using a neural network. NeuralCoref is production-ready, integrated in spaCy's NLP pipeline and easily extensible to new training datasets.
                </p>
                <p>
NeuralCoref is written in Cython & Python and comes with pre-trained statistical models for English. It can be trained in other languages.
                </p>
              </p>
            </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/torchmoji.jpeg"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://github.com/huggingface/torchMoji">
                  <papertitle>üòá TorchMoji</papertitle>
                </a>
                <p>
                  TorchMoji is a pyTorch implementation of the <a href="https://github.com/bfelbo/DeepMoji">DeepMoji</a> model developped by Bjarke Felbo, Alan Mislove, Anders S√∏gaard, Iyad Rahwan and Sune Lehmann.

                  This model trained on 1.2 billion tweets with emojis to understand how language is used to express emotions. Through transfer learning the model can obtain state-of-the-art performance on many emotion-related text modeling tasks. See the <a href="https://arxiv.org/abs/1708.00524">paper</a> for more details.
                  
                                  </p>
              </p>
            </td>
          </tr>

        <tr>
          <td width="25%"><img src="images/openai_gpt.png"></td>
          <td width="75%" valign="top">
            <p>
              <a href="https://github.com/huggingface/pytorch-openai-transformer-lm">
                <papertitle>PyTorch implementation of OpenAI's Finetuned Transformer Language Model.</papertitle>
              </a>
              <p>
                A PyTorch implementation of the <a href="https://github.com/openai/finetune-transformer-lm">TensorFlow code</a> provided with OpenAI's paper <a href="https://blog.openai.com/language-unsupervised">Improving Language Understanding by Generative Pre-Training</a> by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.

                This implementation comprises a script to load in the PyTorch model the weights pre-trained by the authors with the TensorFlow implementation.
                
              </p>
            </p>
          </td>
        </tr>
  
        
        <tr>
          <td width="25%"><img src="images/bert.png"></td>
          <td width="75%" valign="top">
            <p>
              <a href="https://github.com/huggingface/pytorch-pretrained-BERT">
                <papertitle>PyTorch Pretrained Bert.</papertitle>
              </a>
              <p>
                This repository contains an op-for-op PyTorch reimplementation of Google's <a href="https://github.com/google-research/bert">TensorFlow repository</a> for the BERT model that was released together with the paper <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.

This implementation is provided with <a href="https://github.com/google-research/bert">Google's pre-trained models</a>, examples, notebooks and a command-line interface to load any pre-trained TensorFlow checkpoint for BERT is also provided.
              </p>
            </p>
          </td>
        </tr>

      </table>

      


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
            <heading>Publications</heading>
          </td>
        </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
    
        <tr>
          <td width="25%"><img src="images/publi_metalearning.png"></td>
          <td width="75%" valign="top">
            <p>
              <a href="https://arxiv.org/abs/1803.10631">
                <papertitle>Meta-Learning a Dynamical Language Model</papertitle>
              </a>
              <br>
              <strong>Thomas Wolf</strong>, Julien Chaumond, Clement Delangue, 2018
              <br>
              <venue>Workshop track - ICLR 2018</venue>
              </p>
              <p>
                <br> We consider the task of word-level language modeling and study the possibility of combining hidden-states-based short-term representations with medium-term representations encoded in dynamical weights of a language model. Our work extends recent experiments on language models with dynamically evolving weights by casting the language modeling problem into an online learning-to-learn framework in which a meta-learner is trained by gradient-descent to continuously update a language model weights.
              </p>
            </p>
          </td>
        </tr>

        <tr>
            <td width="25%"><img src="images/publi_metalearning_acl.png"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://arxiv.org/abs/1805.05758">
                  <papertitle>Continuous Learning in a Hierarchical Multiscale Neural Network
                    </papertitle>
                </a>
                <br>
                <strong>Thomas Wolf</strong>, Julien Chaumond, Clement Delangue, 2018
                <br>
                <venue>ACL 2018</venue>
                </p>
                <p>
                  <br> We reformulate the problem of encoding a multi-scale representation of a sequence in a language model by casting it in a continuous learning framework. We propose a hierarchical multi-scale language model in which short time-scale dependencies are encoded in the hidden state of a lower-level recurrent neural network while longer time-scale dependencies are encoded in the dynamic of the lower-level network by having a meta-learner update the weights of the lower-level neural network in an online meta-learning fashion. We use elastic weights consolidation as a higher-level to prevent catastrophic forgetting in our continuous learning framework.

                </p>
              </p>
            </td>
          </tr>
  

          <tr>
              <td width="25%"><img src="images/publi_hmtl.png"></td>
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/abs/1811.06031">
                    <papertitle>A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks
                      </papertitle>
                  </a>
                  <br>
                  Victor Sanh, <strong>Thomas Wolf</strong>, Sebastian Ruder
                  <br>
                  <venue>AAAI 2019</venue>
                  </p>
                  <p>
                    <br> Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.
  
                  </p>
                </p>
              </td>
            </tr>

            <tr>
                <td width="25%"><img src="images/publi_TransferTransfo.png"></td>
                <td width="75%" valign="top">
                  <p>
                    <a href="http://alborz-geramifard.com/workshops/nips18-Conversational-AI/Main.html">
                      <papertitle>TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents</papertitle>
                    </a>
                    <br>
                    <strong>Thomas Wolf</strong>, Victor Sanh, Julien Chaumond and Clement Delangue
                    <br>
                    <venue>NeurIPS CAI workshop 2018</venue>
                      <p>
                      <br> We introduce a new approach to data-driven dialogue systems (e.g. chatbots) called TransferTransfo which is a combination of aTransferlearning based train-ing scheme and a high-capacity generativeTransfo-rmer model.  Fine-tuning isperformed by using a multi-task objective which combines several unsupervised pre-diction tasks. The resulting fine-tuned model shows strong improvements over thecurrent state-of-the-art end-to-end conversational models like memory augmentedseq2seq and information-retrieval models. On the privately held PERSONA-CHATdataset of the Conversational Intelligence Challenge 2, this approach obtains anew state-of-the-art, respectively pushing the perplexity, Hits@1 and F1 metrics to 16.28 (45% absolute improvement),80.7 (46% absolute improvement) and 19.5 (20% absolute improvement).
                    </p>
                  </p>
                </td>
              </tr>

      </table>




        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  <a href="https://github.com/jonbarron/jonbarron_website"><strong>Created from Jonathan T. Barron's template</strong></a>
                  </font>
              </p>
            </td>
          </tr>
        </table>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
